{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from normalizer import counter, subSample\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = 'cpu' # Change this to cuda for GPU enabled computers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of our fields have names, but we need them in numbers, so define a dictionary to convert.\n",
    "global magDict\n",
    "magDict = {\n",
    "    'TOTUSJH': 0,\n",
    "    'TOTBSQ': 1,\n",
    "    'TOTPOT': 2,\n",
    "    'TOTUSJZ': 3,\n",
    "    'ABSNJZH': 4,\n",
    "    'SAVNCPP': 5,\n",
    "    'USFLUX': 6,\n",
    "    'TOTFZ': 7,\n",
    "    'MEANPOT': 8,\n",
    "    'EPSZ': 9,\n",
    "    'SHRGT45': 10,\n",
    "    'MEANSHR': 11,\n",
    "    'MEANGAM': 12,\n",
    "    'MEANGBT': 13,\n",
    "    'MEANGBZ': 14,\n",
    "    'MEANGBH': 15,\n",
    "    'MEANJZH': 16,\n",
    "    'TOTFY': 17,\n",
    "    'MEANJZD': 18,\n",
    "    'MEANALP': 19,\n",
    "    'TOTFX': 20,\n",
    "    'EPSY': 21,\n",
    "    'EPSX': 22,\n",
    "    'R_VALUE': 23,\n",
    "    'RBZ_VALUE': 24,\n",
    "    'RBT_VALUE': 25,\n",
    "    'RBP_VALUE': 26,\n",
    "    'FDIM': 27,\n",
    "    'BZ_FDIM': 28,\n",
    "    'BT_FDIM': 29,\n",
    "    'BP_FDIM': 30,\n",
    "    'PIL_LEN': 31,\n",
    "    'XR_MAX': 32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data from the JSON file, then return it as a tensor of input data and a list of labels\n",
    "def getDataFromJSON(path=\"data/train_partition1_data.json\", device='cpu', earlyStop=-1):\n",
    "    # path is the path to the files, device is where to store it (CUDA), earlyStop is how many lines to \n",
    "    # read if you don't want the entire file read.\n",
    "    \n",
    "    # Get the dictionary to assign names to numbers\n",
    "    global magDict\n",
    "    \n",
    "    # This dataset is heavily skewed, so we need to get the number of each type of flare.\n",
    "    # This also lets us get the number of lines in the file with a sum.\n",
    "    # This function also ignores any lines with a value of NaN.\n",
    "    weights = counter(path, earlyStop)\n",
    "    lines = np.sum(weights)\n",
    "    # Check when we want to stop - the end of the file or earlier.\n",
    "    if earlyStop < 0: length = lines\n",
    "    else: length = min(earlyStop, lines)\n",
    "    \n",
    "    # Get the file and open it. \n",
    "    file = open(path)    \n",
    "    \n",
    "    # Declare a tensor to hold the data, and a list to hold the labels.\n",
    "    # Dimensions: 0: number of entries we want. 1: the 33 fields in the data. 2: the 60 observations in each field. \n",
    "    tnsr = torch.Tensor().new_empty((length, 33, 60), device=device)\n",
    "    labels = []\n",
    "    flares = {'X':0, 'M':1, 'C':2, 'B':3, 'Q':4}\n",
    "        \n",
    "    row = -1\n",
    "    for line in file:\n",
    "        if 'nan' in line or \"NaN\" in line:\n",
    "            continue\n",
    "        # Load the line as a dictionary. Row is an integer place and v is a smaller dictionary.\n",
    "        d: dict = json.loads(line)\n",
    "        row += 1\n",
    "        for _, v in d.items(): # we use the _ because we don't want the ID.\n",
    "            if earlyStop > 0 and row >= earlyStop:\n",
    "                # If we don't want the entire dataset, stop loading more than we want\n",
    "                return tnsr, labels, weights\n",
    "            if row % 100 == 0:\n",
    "                print(f'Now loading event {row}/{length}')\n",
    "            # append the label to our list\n",
    "            labels.append(flares[v['label']])\n",
    "            \n",
    "            # Break each individual dictionary into dictionaries of observations\n",
    "            # Key is the string in magDict, and timeDict is a dictionary of observations over time\n",
    "            for key, timeDict in v['values'].items():\n",
    "                # Turn our name string into a numeric value\n",
    "                location = magDict[key]\n",
    "                # Get the measurements out of the time series dictionary\n",
    "                for timeStamp, measurement in timeDict.items():\n",
    "                    tnsr[row][location][int(timeStamp)] = measurement\n",
    "    print(f'{row} lines loaded.')\n",
    "    # Close the file. I'm not a heathen                    \n",
    "    file.close()\n",
    "    # This might be a good place to perform some post processing, but that's a question for another day.\n",
    "    # Famous last words.\n",
    "    return tnsr, labels, weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now loading event 1/785\n",
      "Now loading event 101/785\n",
      "Now loading event 201/785\n",
      "Now loading event 301/785\n",
      "Now loading event 401/785\n",
      "Now loading event 501/785\n",
      "Now loading event 601/785\n",
      "Now loading event 701/785\n",
      "785 lines loaded.\n"
     ]
    }
   ],
   "source": [
    "# This file has 77270 data points. 71633 do not have NAN values.\n",
    "# %time train1, labels1, weights1 = getDataFromJSON(path=\"data/train_partition1_data.json\", earlyStop=-1)\n",
    "train1, labels1 = subSample(\"data/train_partition1_data.json\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now loading event 1/300\n",
      "Now loading event 101/300\n",
      "Now loading event 201/300\n",
      "300 lines loaded.\n"
     ]
    }
   ],
   "source": [
    "# This file has 93767 data points. 82425 of them do not have NAN values.  \n",
    "# %time train2, labels2, weights2 = getDataFromJSON(path=\"data/train_partition2_data.json\", earlyStop=-1)\n",
    "train2, labels2 = subSample(\"data/train_partition2_data.json\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now loading event 1/585\n",
      "Now loading event 101/585\n",
      "Now loading event 201/585\n",
      "Now loading event 301/585\n",
      "Now loading event 401/585\n",
      "Now loading event 501/585\n",
      "585 lines loaded.\n"
     ]
    }
   ],
   "source": [
    "# This file has 42986 data points. 37759 of them do not have NAN values.  \n",
    "# %time train3, labels3, weights3 = getDataFromJSON(path=\"data/train_partition3_data.json\", earlyStop=-1)\n",
    "train3, labels3 = subSample(\"data/train_partition3_data.json\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4318.6987, 4309.0054, 4307.6606, 4301.8848, 4288.6157, 4314.5142,\n",
       "        4359.2119, 4366.2197, 4366.2666, 4393.8687, 4408.9658, 4407.9067,\n",
       "        4431.2881, 4476.7900, 4461.4048, 4525.5620, 4545.3149, 4509.4556,\n",
       "        4566.3525, 4539.2534, 4602.8027, 4594.4609, 4604.9976, 4609.1436,\n",
       "        4598.2563, 4632.2480, 4619.8936, 4598.3965, 4615.3745, 4660.2656,\n",
       "        4617.4736, 4595.8281, 4613.2896, 4588.9224, 4618.5352, 4571.0889,\n",
       "        4555.6548, 4569.3770, 4567.5093, 4578.7480, 4631.6655, 4664.3589,\n",
       "        4703.3726, 4676.2563, 4676.0078, 4704.5059, 4693.2700, 4714.9692,\n",
       "        4821.5654, 4722.6094, 4659.8555, 4695.3633, 4758.1704, 4780.7847,\n",
       "        4715.7080, 4751.1899, 4770.4849, 4760.3877, 4767.5723, 4788.1909])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train2[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network. Make sure to end with nn.Softmax activation\n",
    "import torch.nn as nn\n",
    "from skorch import NeuralNet\n",
    "\n",
    "class logRegWithHidden(nn.Module):\n",
    "    def __init__(self, hidden_size1, hidden_size2, num_classes=5, drop1=.5, input_size=1980):\n",
    "        super().__init__() \n",
    "        self.layer1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.layer2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.layerout = nn.Linear(hidden_size2, num_classes)\n",
    "        #Define a RELU Activation unit\n",
    "        self.relu = nn.ReLU()  \n",
    "        self.smax = nn.Softmax(dim=1)\n",
    "        self.drop = nn.Dropout(p=drop1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Forward Propagate through the layers as defined above\n",
    "        y = self.drop(x.reshape(-1, 1980))\n",
    "        y = self.drop(self.relu(self.layer1(y)))\n",
    "        y = self.relu(self.layer2(y))\n",
    "        y = self.smax(self.layerout(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, inputs, labels, weight, valSets, valLabels, valweight, lr=0.01):\n",
    "    # TODO: Is this right? How do I determine the weights here?\n",
    "    if weight is not None: weight = torch.Tensor(weight)\n",
    "    lfc = nn.CrossEntropyLoss(weight=weight)\n",
    "#     valLabels = torch.tensor(valLabels, dtype=torch.int)\n",
    "    #ideas\n",
    "    # 1-(weight/np.sum(weight))\n",
    "    # .2/weight - this one normalizes so that each class is responsible for 20% of the loss\n",
    "    # 1/weight - this is a bit naive, but the classes with fewer items are weighted more.\n",
    "    # 1/(weight+1) - makes sure we don't have any pesky zeroes\n",
    "    # np.sum(weight)/weight if your learning rate is too low.\n",
    "    \n",
    "    # Hyperparameters\n",
    "    batch = 256\n",
    "    epochs = 4 # artificially low for debugging\n",
    "    \n",
    "    # Start a dataloader object\n",
    "    data = list(zip(inputs,labels))\n",
    "    val = list(zip(valSets,valLabels))\n",
    "    loader = DataLoader(data, batch_size = batch, num_workers=4)\n",
    "    valLoader = DataLoader(val, batch_size = int(len(val)/4), num_workers=4)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    print(opt.state_dict())\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        batch_loss = []\n",
    "        for (xtrain,ytrain) in loader:\n",
    "            opt.zero_grad()\n",
    "            output = model(xtrain)\n",
    "            loss = lfc(output,ytrain)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            batch_loss.append(loss.item())\n",
    "        print(f'The training loss for epoch {epoch+1}/{epochs} was {np.mean(batch_loss)}')\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        balanced = [[],[],[],[],[]]\n",
    "        batchLoss = []\n",
    "        unbalanced = []\n",
    "        \n",
    "        for (xval,yval) in valLoader:\n",
    "            output = model(xval)\n",
    "            loss = lfc(output,yval)\n",
    "            batchLoss.append(loss.item())\n",
    "            corrects = yval.clone().detach() == torch.argmax(output)\n",
    "            unbalanced.append(np.mean([1 if correct else 0 for correct in corrects.detach()]))\n",
    "            \n",
    "            for i, ans in enumerate(yval):\n",
    "                balanced[ans].append(corrects[i])\n",
    "        \n",
    "        balanced = [np.mean(i) for i in balanced]\n",
    "        balancedAccuracy = np.mean(balanced)\n",
    "        \n",
    "#         print(f'The total balanced accuracy for validation was {balancedAccuracy}')\n",
    "        print(f'The unbalanced validation accuracy is {np.mean(unbalanced)}')\n",
    "        print(f'The accuracy for each is {balanced}')           \n",
    "        print(f'The validation loss was :   {epoch+1}/{epochs} was {np.mean(batchLoss)}')\n",
    "    print(opt.state_dict())\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = logRegWithHidden(4096*4, 2048*4, drop1=0).to(device)\n",
    "    # print(weights1+weights2, 1.0/np.array(weights1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logRegWithHidden(\n",
      "  (layer1): Linear(in_features=1980, out_features=16384, bias=True)\n",
      "  (layer2): Linear(in_features=16384, out_features=8192, bias=True)\n",
      "  (layerout): Linear(in_features=8192, out_features=5, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (smax): Softmax(dim=1)\n",
      "  (drop): Dropout(p=0, inplace=False)\n",
      ")\n",
      "{'state': {}, 'param_groups': [{'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5]}]}\n",
      "The training loss for epoch 1/10 was 1.6656454086303711\n",
      "The unbalanced validation accuracy is 0.14794520547945206\n",
      "The accuracy for each is [0.0, 0.0, 0.0, 0.47863247863247865, 0.4444444444444444]\n",
      "The validation loss was :   1/10 was 1.751406764984131\n",
      "The training loss for epoch 2/10 was 1.6656454086303711\n",
      "The unbalanced validation accuracy is 0.14794520547945206\n",
      "The accuracy for each is [0.0, 0.0, 0.0, 0.47863247863247865, 0.4444444444444444]\n",
      "The validation loss was :   2/10 was 1.751406764984131\n",
      "The training loss for epoch 3/10 was 1.6656454086303711\n",
      "The unbalanced validation accuracy is 0.14794520547945206\n",
      "The accuracy for each is [0.0, 0.0, 0.0, 0.47863247863247865, 0.4444444444444444]\n",
      "The validation loss was :   3/10 was 1.751406764984131\n",
      "The training loss for epoch 4/10 was 1.6656454086303711\n",
      "The unbalanced validation accuracy is 0.14794520547945206\n",
      "The accuracy for each is [0.0, 0.0, 0.0, 0.47863247863247865, 0.4444444444444444]\n",
      "The validation loss was :   4/10 was 1.751406764984131\n",
      "The training loss for epoch 5/10 was 1.6656454086303711\n",
      "The unbalanced validation accuracy is 0.14794520547945206\n",
      "The accuracy for each is [0.0, 0.0, 0.0, 0.47863247863247865, 0.4444444444444444]\n",
      "The validation loss was :   5/10 was 1.751406764984131\n",
      "The training loss for epoch 6/10 was 1.6656454086303711\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "# [weights1[i] + weights2[i] for i in range(5)]\n",
    "%time newModel = train(model, torch.cat((train1, train2), dim=0), labels1 + labels2, None, train3, labels3, None, lr = 0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=4096, out_features=2048, bias=True)\n",
      "Linear(in_features=4096, out_features=2048, bias=True)\n",
      "tensor(0.)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "s = 1 # choose this to check the sth layer\n",
    "feature_extraction1 = [child for child in model.children()]\n",
    "print(feature_extraction1[s])\n",
    "feature_extraction2 = [child for child in newModel.children()]\n",
    "print(feature_extraction2[s])\n",
    "print(torch.max(feature_extraction1[s].weight - feature_extraction2[s].weight).detach())\n",
    "print(torch.min(feature_extraction1[s].weight - feature_extraction2[s].weight).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss for epoch 1/10 was 1.690267562866211\n",
      "The unbalanced validation accuracy is 0.18666666666666668\n",
      "The accuracy for each is [0.0, 0.0, 0.75, 0.0, 0.18333333333333332]\n",
      "The validation loss was :   1/10 was 1.7014981508255005\n",
      "The training loss for epoch 2/10 was 1.690267562866211\n",
      "The unbalanced validation accuracy is 0.18666666666666668\n",
      "The accuracy for each is [0.0, 0.0, 0.75, 0.0, 0.18333333333333332]\n",
      "The validation loss was :   2/10 was 1.7014981508255005\n",
      "The training loss for epoch 3/10 was 1.690267562866211\n",
      "The unbalanced validation accuracy is 0.18666666666666668\n",
      "The accuracy for each is [0.0, 0.0, 0.75, 0.0, 0.18333333333333332]\n",
      "The validation loss was :   3/10 was 1.7014981508255005\n",
      "The training loss for epoch 4/10 was 1.690267562866211\n",
      "The unbalanced validation accuracy is 0.18666666666666668\n",
      "The accuracy for each is [0.0, 0.0, 0.75, 0.0, 0.18333333333333332]\n",
      "The validation loss was :   4/10 was 1.7014981508255005\n",
      "The training loss for epoch 5/10 was 1.690267562866211\n",
      "The unbalanced validation accuracy is 0.18666666666666668\n",
      "The accuracy for each is [0.0, 0.0, 0.75, 0.0, 0.18333333333333332]\n",
      "The validation loss was :   5/10 was 1.7014981508255005\n",
      "The training loss for epoch 6/10 was 1.690267562866211\n",
      "The unbalanced validation accuracy is 0.18666666666666668\n",
      "The accuracy for each is [0.0, 0.0, 0.75, 0.0, 0.18333333333333332]\n",
      "The validation loss was :   6/10 was 1.7014981508255005\n",
      "The training loss for epoch 7/10 was 1.690267562866211\n",
      "The unbalanced validation accuracy is 0.18666666666666668\n",
      "The accuracy for each is [0.0, 0.0, 0.75, 0.0, 0.18333333333333332]\n",
      "The validation loss was :   7/10 was 1.7014981508255005\n",
      "The training loss for epoch 8/10 was 1.690267562866211\n",
      "The unbalanced validation accuracy is 0.18666666666666668\n",
      "The accuracy for each is [0.0, 0.0, 0.75, 0.0, 0.18333333333333332]\n",
      "The validation loss was :   8/10 was 1.7014981508255005\n",
      "The training loss for epoch 9/10 was 1.690267562866211\n",
      "The unbalanced validation accuracy is 0.18666666666666668\n",
      "The accuracy for each is [0.0, 0.0, 0.75, 0.0, 0.18333333333333332]\n",
      "The validation loss was :   9/10 was 1.7014981508255005\n",
      "The training loss for epoch 10/10 was 1.690267562866211\n",
      "The unbalanced validation accuracy is 0.18666666666666668\n",
      "The accuracy for each is [0.0, 0.0, 0.75, 0.0, 0.18333333333333332]\n",
      "The validation loss was :   10/10 was 1.7014981508255005\n"
     ]
    }
   ],
   "source": [
    "_ = train(\n",
    "    model,\n",
    "    torch.cat((train1, train3), dim=0),\n",
    "    labels1 + labels3,\n",
    "    None,\n",
    "    train2,\n",
    "    labels2,\n",
    "    None,\n",
    "    lr = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training loss for epoch 1/10 was 1.7027375102043152\n",
      "The unbalanced validation accuracy is 0.16122448979591836\n",
      "The accuracy for each is [0.0, 0.0, 0.7770700636942676, 0.0, 0.22929936305732485]\n",
      "The validation loss was :   1/10 was 1.733405590057373\n",
      "The training loss for epoch 2/10 was 1.7027375102043152\n",
      "The unbalanced validation accuracy is 0.16122448979591836\n",
      "The accuracy for each is [0.0, 0.0, 0.7770700636942676, 0.0, 0.22929936305732485]\n",
      "The validation loss was :   2/10 was 1.733405590057373\n",
      "The training loss for epoch 3/10 was 1.7027375102043152\n",
      "The unbalanced validation accuracy is 0.16122448979591836\n",
      "The accuracy for each is [0.0, 0.0, 0.7770700636942676, 0.0, 0.22929936305732485]\n",
      "The validation loss was :   3/10 was 1.733405590057373\n",
      "The training loss for epoch 4/10 was 1.7027375102043152\n",
      "The unbalanced validation accuracy is 0.16122448979591836\n",
      "The accuracy for each is [0.0, 0.0, 0.7770700636942676, 0.0, 0.22929936305732485]\n",
      "The validation loss was :   4/10 was 1.733405590057373\n",
      "The training loss for epoch 5/10 was 1.7027375102043152\n",
      "The unbalanced validation accuracy is 0.16122448979591836\n",
      "The accuracy for each is [0.0, 0.0, 0.7770700636942676, 0.0, 0.22929936305732485]\n",
      "The validation loss was :   5/10 was 1.733405590057373\n",
      "The training loss for epoch 6/10 was 1.7027375102043152\n",
      "The unbalanced validation accuracy is 0.16122448979591836\n",
      "The accuracy for each is [0.0, 0.0, 0.7770700636942676, 0.0, 0.22929936305732485]\n",
      "The validation loss was :   6/10 was 1.733405590057373\n",
      "The training loss for epoch 7/10 was 1.7027375102043152\n",
      "The unbalanced validation accuracy is 0.16122448979591836\n",
      "The accuracy for each is [0.0, 0.0, 0.7770700636942676, 0.0, 0.22929936305732485]\n",
      "The validation loss was :   7/10 was 1.733405590057373\n",
      "The training loss for epoch 8/10 was 1.7027375102043152\n",
      "The unbalanced validation accuracy is 0.16122448979591836\n",
      "The accuracy for each is [0.0, 0.0, 0.7770700636942676, 0.0, 0.22929936305732485]\n",
      "The validation loss was :   8/10 was 1.733405590057373\n",
      "The training loss for epoch 9/10 was 1.7027375102043152\n",
      "The unbalanced validation accuracy is 0.16122448979591836\n",
      "The accuracy for each is [0.0, 0.0, 0.7770700636942676, 0.0, 0.22929936305732485]\n",
      "The validation loss was :   9/10 was 1.733405590057373\n",
      "The training loss for epoch 10/10 was 1.7027375102043152\n",
      "The unbalanced validation accuracy is 0.16122448979591836\n",
      "The accuracy for each is [0.0, 0.0, 0.7770700636942676, 0.0, 0.22929936305732485]\n",
      "The validation loss was :   10/10 was 1.733405590057373\n"
     ]
    }
   ],
   "source": [
    "_ = train(\n",
    "    model,\n",
    "    torch.cat((train3, train2), dim=0),\n",
    "    labels3 + labels2,\n",
    "    None,\n",
    "    train1,\n",
    "    labels1,\n",
    "    None,\n",
    "    lr = 0.00000001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
