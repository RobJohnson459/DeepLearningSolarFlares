{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from normalizer import counter\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of our fields have names, but we need them in numbers, so define a dictionary to convert.\n",
    "global magDict\n",
    "magDict = {\n",
    "    'TOTUSJH': 0,\n",
    "    'TOTBSQ': 1,\n",
    "    'TOTPOT': 2,\n",
    "    'TOTUSJZ': 3,\n",
    "    'ABSNJZH': 4,\n",
    "    'SAVNCPP': 5,\n",
    "    'USFLUX': 6,\n",
    "    'TOTFZ': 7,\n",
    "    'MEANPOT': 8,\n",
    "    'EPSZ': 9,\n",
    "    'SHRGT45': 10,\n",
    "    'MEANSHR': 11,\n",
    "    'MEANGAM': 12,\n",
    "    'MEANGBT': 13,\n",
    "    'MEANGBZ': 14,\n",
    "    'MEANGBH': 15,\n",
    "    'MEANJZH': 16,\n",
    "    'TOTFY': 17,\n",
    "    'MEANJZD': 18,\n",
    "    'MEANALP': 19,\n",
    "    'TOTFX': 20,\n",
    "    'EPSY': 21,\n",
    "    'EPSX': 22,\n",
    "    'R_VALUE': 23,\n",
    "    'RBZ_VALUE': 24,\n",
    "    'RBT_VALUE': 25,\n",
    "    'RBP_VALUE': 26,\n",
    "    'FDIM': 27,\n",
    "    'BZ_FDIM': 28,\n",
    "    'BT_FDIM': 29,\n",
    "    'BP_FDIM': 30,\n",
    "    'PIL_LEN': 31,\n",
    "    'XR_MAX': 32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data from the JSON file, then return it as a tensor of input data and a list of labels\n",
    "def getDataFromJSON(path=\"data/train_partition1_data.json\", device='cpu', earlyStop=-1):\n",
    "    # I might need to refactor these arguments to allow to get the test data. Problem for another day.\n",
    "    # path is the path to the files, device is where to store it (CUDA), earlyStop is how many lines to \n",
    "    # read if you don't want the entire file read.\n",
    "    \n",
    "    # Get the dictionary to assign names to numbers\n",
    "    global magDict\n",
    "    \n",
    "    # This dataset is heavily skewed, so we need to get the number of each type of flare.\n",
    "    # This also lets us get the number of lines in the file with a sum.\n",
    "    weights = counter(path, earlyStop)\n",
    "    lines = np.sum(weights)\n",
    "    # Check when we want to stop - the end of the file or earlier.\n",
    "    if earlyStop < 0: length = lines\n",
    "    else: length = min(earlyStop, lines)\n",
    "    \n",
    "    # Get the file and open it. \n",
    "    file = open(path)    \n",
    "    \n",
    "    # Declare a tensor to hold the data, and a list to hold the labels.\n",
    "    # Dimensions: 0: number of entries we want. 1: the 33 fields in the data. 2: the 60 observations in each field. \n",
    "    tnsr = torch.Tensor().new_empty((length, 33, 60), device=device)\n",
    "    labels = []\n",
    "    flares = {'X':0, 'M':1, 'C':2, 'B':3, 'Q':4}\n",
    "        \n",
    "    row = -1\n",
    "    for line in file:\n",
    "        # Load the line as a dictionary. Row is an integer place and v is a smaller dictionary.\n",
    "        d: dict = json.loads(line)\n",
    "        row += 1\n",
    "        for _, v in d.items(): # we use the _ because we don't want the ID.\n",
    "            if earlyStop > 0 and row >= earlyStop:\n",
    "                # If we don't want the entire dataset, stop loading more than we want\n",
    "                return tnsr, labels, weights\n",
    "            if row % 100 == 0:\n",
    "                print(f'Now loading event {row}/{length}')\n",
    "            # append the label to our list\n",
    "            labels.append(flares[v['label']])\n",
    "            \n",
    "            # Break each individual dictionary into dictionaries of observations\n",
    "            # Key is the string in magDict, and timeDict is a dictionary of observations over time\n",
    "            for key, timeDict in v['values'].items():\n",
    "                # Turn our name string into a numeric value\n",
    "                location = magDict[key]\n",
    "                # Get the measurements out of the time series dictionary\n",
    "                for timeStamp, measurement in timeDict.items():\n",
    "                    tnsr[row][location][int(timeStamp)] = measurement\n",
    "    print(f'{row} lines loaded.')\n",
    "    # Close the file. I'm not a heathen                    \n",
    "    file.close()\n",
    "    # This might be a good place to perform some post processing, but that's a question for another day.\n",
    "    # Famous last words.\n",
    "    return tnsr, labels, weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now loading event 0/10000\n",
      "Now loading event 100/10000\n",
      "Now loading event 200/10000\n",
      "Now loading event 300/10000\n",
      "Now loading event 400/10000\n",
      "Now loading event 500/10000\n",
      "Now loading event 600/10000\n",
      "Now loading event 700/10000\n",
      "Now loading event 800/10000\n",
      "Now loading event 900/10000\n",
      "Now loading event 1000/10000\n",
      "Now loading event 1100/10000\n",
      "Now loading event 1200/10000\n",
      "Now loading event 1300/10000\n",
      "Now loading event 1400/10000\n",
      "Now loading event 1500/10000\n",
      "Now loading event 1600/10000\n",
      "Now loading event 1700/10000\n",
      "Now loading event 1800/10000\n",
      "Now loading event 1900/10000\n",
      "Now loading event 2000/10000\n",
      "Now loading event 2100/10000\n",
      "Now loading event 2200/10000\n",
      "Now loading event 2300/10000\n",
      "Now loading event 2400/10000\n",
      "Now loading event 2500/10000\n",
      "Now loading event 2600/10000\n",
      "Now loading event 2700/10000\n",
      "Now loading event 2800/10000\n",
      "Now loading event 2900/10000\n",
      "Now loading event 3000/10000\n",
      "Now loading event 3100/10000\n",
      "Now loading event 3200/10000\n",
      "Now loading event 3300/10000\n",
      "Now loading event 3400/10000\n",
      "Now loading event 3500/10000\n",
      "Now loading event 3600/10000\n",
      "Now loading event 3700/10000\n",
      "Now loading event 3800/10000\n",
      "Now loading event 3900/10000\n",
      "Now loading event 4000/10000\n",
      "Now loading event 4100/10000\n",
      "Now loading event 4200/10000\n",
      "Now loading event 4300/10000\n",
      "Now loading event 4400/10000\n",
      "Now loading event 4500/10000\n",
      "Now loading event 4600/10000\n",
      "Now loading event 4700/10000\n",
      "Now loading event 4800/10000\n",
      "Now loading event 4900/10000\n",
      "Now loading event 5000/10000\n",
      "Now loading event 5100/10000\n",
      "Now loading event 5200/10000\n",
      "Now loading event 5300/10000\n",
      "Now loading event 5400/10000\n",
      "Now loading event 5500/10000\n",
      "Now loading event 5600/10000\n",
      "Now loading event 5700/10000\n",
      "Now loading event 5800/10000\n",
      "Now loading event 5900/10000\n",
      "Now loading event 6000/10000\n",
      "Now loading event 6100/10000\n",
      "Now loading event 6200/10000\n",
      "Now loading event 6300/10000\n",
      "Now loading event 6400/10000\n",
      "Now loading event 6500/10000\n",
      "Now loading event 6600/10000\n",
      "Now loading event 6700/10000\n",
      "Now loading event 6800/10000\n",
      "Now loading event 6900/10000\n",
      "Now loading event 7000/10000\n",
      "Now loading event 7100/10000\n",
      "Now loading event 7200/10000\n",
      "Now loading event 7300/10000\n",
      "Now loading event 7400/10000\n",
      "Now loading event 7500/10000\n",
      "Now loading event 7600/10000\n",
      "Now loading event 7700/10000\n",
      "Now loading event 7800/10000\n",
      "Now loading event 7900/10000\n",
      "Now loading event 8000/10000\n",
      "Now loading event 8100/10000\n",
      "Now loading event 8200/10000\n",
      "Now loading event 8300/10000\n",
      "Now loading event 8400/10000\n",
      "Now loading event 8500/10000\n",
      "Now loading event 8600/10000\n",
      "Now loading event 8700/10000\n",
      "Now loading event 8800/10000\n",
      "Now loading event 8900/10000\n",
      "Now loading event 9000/10000\n",
      "Now loading event 9100/10000\n",
      "Now loading event 9200/10000\n",
      "Now loading event 9300/10000\n",
      "Now loading event 9400/10000\n",
      "Now loading event 9500/10000\n",
      "Now loading event 9600/10000\n",
      "Now loading event 9700/10000\n",
      "Now loading event 9800/10000\n",
      "Now loading event 9900/10000\n",
      "CPU times: user 1min 56s, sys: 1.22 s, total: 1min 57s\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "# This file has 77270 data points. \n",
    "%time train1, labels1, weights1 = getDataFromJSON(path=\"data/train_partition1_data.json\", earlyStop=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now loading event 0/10000\n",
      "Now loading event 100/10000\n",
      "Now loading event 200/10000\n",
      "Now loading event 300/10000\n",
      "Now loading event 400/10000\n",
      "Now loading event 500/10000\n",
      "Now loading event 600/10000\n",
      "Now loading event 700/10000\n",
      "Now loading event 800/10000\n",
      "Now loading event 900/10000\n",
      "Now loading event 1000/10000\n",
      "Now loading event 1100/10000\n",
      "Now loading event 1200/10000\n",
      "Now loading event 1300/10000\n",
      "Now loading event 1400/10000\n",
      "Now loading event 1500/10000\n",
      "Now loading event 1600/10000\n",
      "Now loading event 1700/10000\n",
      "Now loading event 1800/10000\n",
      "Now loading event 1900/10000\n",
      "Now loading event 2000/10000\n",
      "Now loading event 2100/10000\n",
      "Now loading event 2200/10000\n",
      "Now loading event 2300/10000\n",
      "Now loading event 2400/10000\n",
      "Now loading event 2500/10000\n",
      "Now loading event 2600/10000\n",
      "Now loading event 2700/10000\n",
      "Now loading event 2800/10000\n",
      "Now loading event 2900/10000\n",
      "Now loading event 3000/10000\n",
      "Now loading event 3100/10000\n",
      "Now loading event 3200/10000\n",
      "Now loading event 3300/10000\n",
      "Now loading event 3400/10000\n",
      "Now loading event 3500/10000\n",
      "Now loading event 3600/10000\n",
      "Now loading event 3700/10000\n",
      "Now loading event 3800/10000\n",
      "Now loading event 3900/10000\n",
      "Now loading event 4000/10000\n",
      "Now loading event 4100/10000\n",
      "Now loading event 4200/10000\n",
      "Now loading event 4300/10000\n",
      "Now loading event 4400/10000\n",
      "Now loading event 4500/10000\n",
      "Now loading event 4600/10000\n",
      "Now loading event 4700/10000\n",
      "Now loading event 4800/10000\n",
      "Now loading event 4900/10000\n",
      "Now loading event 5000/10000\n",
      "Now loading event 5100/10000\n",
      "Now loading event 5200/10000\n",
      "Now loading event 5300/10000\n",
      "Now loading event 5400/10000\n",
      "Now loading event 5500/10000\n",
      "Now loading event 5600/10000\n",
      "Now loading event 5700/10000\n",
      "Now loading event 5800/10000\n",
      "Now loading event 5900/10000\n",
      "Now loading event 6000/10000\n",
      "Now loading event 6100/10000\n",
      "Now loading event 6200/10000\n",
      "Now loading event 6300/10000\n",
      "Now loading event 6400/10000\n",
      "Now loading event 6500/10000\n",
      "Now loading event 6600/10000\n",
      "Now loading event 6700/10000\n",
      "Now loading event 6800/10000\n",
      "Now loading event 6900/10000\n",
      "Now loading event 7000/10000\n",
      "Now loading event 7100/10000\n",
      "Now loading event 7200/10000\n",
      "Now loading event 7300/10000\n",
      "Now loading event 7400/10000\n",
      "Now loading event 7500/10000\n",
      "Now loading event 7600/10000\n",
      "Now loading event 7700/10000\n",
      "Now loading event 7800/10000\n",
      "Now loading event 7900/10000\n",
      "Now loading event 8000/10000\n",
      "Now loading event 8100/10000\n",
      "Now loading event 8200/10000\n",
      "Now loading event 8300/10000\n",
      "Now loading event 8400/10000\n",
      "Now loading event 8500/10000\n",
      "Now loading event 8600/10000\n",
      "Now loading event 8700/10000\n",
      "Now loading event 8800/10000\n",
      "Now loading event 8900/10000\n",
      "Now loading event 9000/10000\n",
      "Now loading event 9100/10000\n",
      "Now loading event 9200/10000\n",
      "Now loading event 9300/10000\n",
      "Now loading event 9400/10000\n",
      "Now loading event 9500/10000\n",
      "Now loading event 9600/10000\n",
      "Now loading event 9700/10000\n",
      "Now loading event 9800/10000\n",
      "Now loading event 9900/10000\n",
      "CPU times: user 1min 57s, sys: 660 ms, total: 1min 58s\n",
      "Wall time: 1min 58s\n"
     ]
    }
   ],
   "source": [
    "# This file has 93767 data points. \n",
    "%time train2, labels2, weights2 = getDataFromJSON(path=\"data/train_partition2_data.json\", earlyStop=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now loading event 0/10000\n",
      "Now loading event 100/10000\n",
      "Now loading event 200/10000\n",
      "Now loading event 300/10000\n",
      "Now loading event 400/10000\n",
      "Now loading event 500/10000\n",
      "Now loading event 600/10000\n",
      "Now loading event 700/10000\n",
      "Now loading event 800/10000\n",
      "Now loading event 900/10000\n",
      "Now loading event 1000/10000\n",
      "Now loading event 1100/10000\n",
      "Now loading event 1200/10000\n",
      "Now loading event 1300/10000\n",
      "Now loading event 1400/10000\n",
      "Now loading event 1500/10000\n",
      "Now loading event 1600/10000\n",
      "Now loading event 1700/10000\n",
      "Now loading event 1800/10000\n",
      "Now loading event 1900/10000\n",
      "Now loading event 2000/10000\n",
      "Now loading event 2100/10000\n",
      "Now loading event 2200/10000\n",
      "Now loading event 2300/10000\n",
      "Now loading event 2400/10000\n",
      "Now loading event 2500/10000\n",
      "Now loading event 2600/10000\n",
      "Now loading event 2700/10000\n",
      "Now loading event 2800/10000\n",
      "Now loading event 2900/10000\n",
      "Now loading event 3000/10000\n",
      "Now loading event 3100/10000\n",
      "Now loading event 3200/10000\n",
      "Now loading event 3300/10000\n",
      "Now loading event 3400/10000\n",
      "Now loading event 3500/10000\n",
      "Now loading event 3600/10000\n",
      "Now loading event 3700/10000\n",
      "Now loading event 3800/10000\n",
      "Now loading event 3900/10000\n",
      "Now loading event 4000/10000\n",
      "Now loading event 4100/10000\n",
      "Now loading event 4200/10000\n",
      "Now loading event 4300/10000\n",
      "Now loading event 4400/10000\n",
      "Now loading event 4500/10000\n",
      "Now loading event 4600/10000\n",
      "Now loading event 4700/10000\n",
      "Now loading event 4800/10000\n",
      "Now loading event 4900/10000\n",
      "Now loading event 5000/10000\n",
      "Now loading event 5100/10000\n",
      "Now loading event 5200/10000\n",
      "Now loading event 5300/10000\n",
      "Now loading event 5400/10000\n",
      "Now loading event 5500/10000\n",
      "Now loading event 5600/10000\n",
      "Now loading event 5700/10000\n",
      "Now loading event 5800/10000\n",
      "Now loading event 5900/10000\n",
      "Now loading event 6000/10000\n",
      "Now loading event 6100/10000\n",
      "Now loading event 6200/10000\n",
      "Now loading event 6300/10000\n",
      "Now loading event 6400/10000\n",
      "Now loading event 6500/10000\n",
      "Now loading event 6600/10000\n",
      "Now loading event 6700/10000\n",
      "Now loading event 6800/10000\n",
      "Now loading event 6900/10000\n",
      "Now loading event 7000/10000\n",
      "Now loading event 7100/10000\n",
      "Now loading event 7200/10000\n",
      "Now loading event 7300/10000\n",
      "Now loading event 7400/10000\n",
      "Now loading event 7500/10000\n",
      "Now loading event 7600/10000\n",
      "Now loading event 7700/10000\n",
      "Now loading event 7800/10000\n",
      "Now loading event 7900/10000\n",
      "Now loading event 8000/10000\n",
      "Now loading event 8100/10000\n",
      "Now loading event 8200/10000\n",
      "Now loading event 8300/10000\n",
      "Now loading event 8400/10000\n",
      "Now loading event 8500/10000\n",
      "Now loading event 8600/10000\n",
      "Now loading event 8700/10000\n",
      "Now loading event 8800/10000\n",
      "Now loading event 8900/10000\n",
      "Now loading event 9000/10000\n",
      "Now loading event 9100/10000\n",
      "Now loading event 9200/10000\n",
      "Now loading event 9300/10000\n",
      "Now loading event 9400/10000\n",
      "Now loading event 9500/10000\n",
      "Now loading event 9600/10000\n",
      "Now loading event 9700/10000\n",
      "Now loading event 9800/10000\n",
      "Now loading event 9900/10000\n",
      "CPU times: user 1min 54s, sys: 568 ms, total: 1min 55s\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "# This file has 42986 data points. \n",
    "%time train3, labels3, weights3 = getDataFromJSON(path=\"data/train_partition3_data.json\", earlyStop=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19.0601, 18.4611, 25.6917, 17.3464, 18.1081, 18.0577, 18.7077, 17.4748,\n",
       "        17.8627, 16.9230, 22.6015, 22.5883, 18.8607, 19.9172, 23.7632, 21.3895,\n",
       "        27.7625, 26.1885, 26.1678, 27.3103, 27.4704, 26.1131, 29.2025, 31.6294,\n",
       "        28.8686, 27.6369, 26.0752, 20.9289, 19.3941, 17.8339, 23.0833, 19.8953,\n",
       "        20.2367, 20.7759, 21.6280, 22.9786, 20.4450, 20.1092, 19.7991, 21.1444,\n",
       "        16.3790, 20.6794, 16.9210, 18.0858, 16.4018, 16.2757, 17.2050, 19.5720,\n",
       "        19.2548, 19.9467, 18.0555, 18.8806, 18.5460, 20.5339, 18.9813, 16.3843,\n",
       "        18.5569, 17.3046, 17.9197, 14.8065])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train2[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network. Make sure to end with nn.Softmax activation\n",
    "import torch.nn as nn\n",
    "from skorch import NeuralNet\n",
    "\n",
    "class simpleModel(nn.Module):\n",
    "    def __init__(self, n, hidden_size, num_classes=5, drop1=.5):\n",
    "        super().__init__() \n",
    "        self.n = n\n",
    "        self.layer1 = []\n",
    "        for _ in range(33):\n",
    "            self.layer1.append(nn.Linear(60,n))\n",
    "        self.layer2 = nn.Linear(n*33, hidden_size)\n",
    "        self.layerout = nn.Linear(hidden_size, num_classes)\n",
    "        #Define a RELU Activation unit\n",
    "        self.relu = nn.ReLU()  \n",
    "        self.smax = nn.Softmax(dim=1)\n",
    "        self.drop = nn.Dropout(p=drop1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Forward Propagate through the layers as defined above\n",
    "        output = torch.empty((x.shape[0], 33, self.n))\n",
    "        for i in range(33):\n",
    "            output[:,i,:]=self.layer1[i](x[:, i, :])\n",
    "            if i == 1: print(x[0,i,:])\n",
    "        print(x)\n",
    "        print('This is iffy')\n",
    "        print(output)\n",
    "        assert False\n",
    "        y = self.relu( output.reshape(-1, 33*self.n))\n",
    "        y = self.relu(self.layer2(y))\n",
    "        y = self.smax(self.layerout(y))\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = simpleModel(1, 30)\n",
    "model3 = simpleModel(3, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, inputs, labels, weight, valSets, valLabels, valweight):\n",
    "    # TODO: Is this right? How do I determine the weights here?\n",
    "    weight = torch.Tensor(weight)\n",
    "    lfc = nn.CrossEntropyLoss(weight=None)\n",
    "#     valLabels = torch.tensor(valLabels, dtype=torch.int)\n",
    "    #ideas\n",
    "    # 1-(weight/np.sum(weight))\n",
    "    # .2/weight - this one normalizes so that each class is responsible for 20% of the loss\n",
    "    # 1/weight - this is a bit naive, but the classes with fewer items are weighted more.\n",
    "    # 1/(weight+1) - makes sure we don't have any pesky zeroes\n",
    "    # np.sum(weight)/weight if your learning rate is too low.\n",
    "    \n",
    "    # Hyperparameters\n",
    "    batch = 256\n",
    "    epochs = 10\n",
    "    lr = 0.01\n",
    "    \n",
    "    # Start a dataloader object\n",
    "    data = list(zip(inputs,labels))\n",
    "    val = list(zip(valSets,valLabels))\n",
    "    loader = DataLoader(data, batch_size = batch, num_workers=4)\n",
    "    valLoader = DataLoader(val, batch_size = int(len(val)/4), num_workers=4)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        batch_loss = []\n",
    "        for (xtrain,ytrain) in loader:\n",
    "            output = model(xtrain)\n",
    "            loss = lfc(output,ytrain)\n",
    "            print(loss)\n",
    "            assert False\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            batch_loss.append(loss.item())\n",
    "        print(f'The training loss for epoch {epoch+1}/{epochs} was {np.mean(batch_loss)}')\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        balanced = [[],[],[],[],[]]\n",
    "        batchLoss = []\n",
    "        unbalanced = []\n",
    "        \n",
    "        for (xval,yval) in valLoader:\n",
    "            output = model(xval)\n",
    "            loss = lfc(output,yval)\n",
    "            batchLoss.append(loss.item())\n",
    "            corrects = yval.clone().detach() == torch.argmax(output)\n",
    "            unbalanced.append(np.mean([1 if correct else 0 for correct in corrects.detach()]))\n",
    "            \n",
    "            # TODO: figure this one out.\n",
    "            for i, ans in enumerate(yval):\n",
    "                balanced[ans].append(corrects[i])\n",
    "        \n",
    "        balanced = [np.mean(i) for i in balanced]\n",
    "        balancedAccuracy = np.mean(balanced)\n",
    "        \n",
    "        print(f'The total balanced accuracy for validation was {balancedAccuracy}')\n",
    "        print(f'The unbalanced validation accuracy is {np.mean(unbalanced)}')\n",
    "        print(f'The accuracy for each is {balanced}')           \n",
    "        print(f'The validation loss was :   {epoch+1}/{epochs} was {np.mean(batchLoss)}')\n",
    "\n",
    "            \n",
    "        print(f'The validation loss was :   {epoch+1}/{epochs} was {np.mean(batchLoss)}')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simpleModel(\n",
      "  (layer2): Linear(in_features=33, out_features=30, bias=True)\n",
      "  (layerout): Linear(in_features=30, out_features=5, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (smax): Softmax(dim=1)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "tensor([2.5324e+10, 2.5299e+10, 2.5383e+10, 2.5641e+10, 2.5717e+10, 2.5705e+10,\n",
      "        2.5911e+10, 2.5993e+10, 2.6150e+10, 2.6238e+10, 2.6213e+10, 2.6242e+10,\n",
      "        2.6259e+10, 2.6233e+10, 2.6227e+10, 2.6129e+10, 2.6101e+10, 2.6040e+10,\n",
      "        2.6038e+10, 2.5739e+10, 2.5711e+10, 2.5693e+10, 2.5609e+10, 2.5570e+10,\n",
      "        2.5561e+10, 2.5571e+10, 2.5564e+10, 2.5549e+10, 2.5484e+10, 2.5468e+10,\n",
      "        2.5498e+10, 2.5502e+10, 2.5365e+10, 2.5313e+10, 2.5229e+10, 2.5170e+10,\n",
      "        2.5025e+10, 2.4924e+10, 2.4778e+10, 2.4476e+10, 2.4336e+10, 2.4267e+10,\n",
      "        2.4127e+10, 2.4175e+10, 2.4057e+10, 2.4005e+10, 2.3994e+10, 2.4100e+10,\n",
      "        2.4046e+10, 2.4052e+10, 2.4056e+10, 2.4023e+10, 2.4093e+10, 2.4187e+10,\n",
      "        2.4107e+10, 2.4106e+10, 2.4065e+10, 2.4020e+10, 2.4003e+10, 2.3926e+10])\n",
      "tensor([[[ 1.2663e+03,  1.2476e+03,  1.2691e+03,  ...,  1.1056e+03,\n",
      "           1.1100e+03,  1.1030e+03],\n",
      "         [ 2.5324e+10,  2.5299e+10,  2.5383e+10,  ...,  2.4020e+10,\n",
      "           2.4003e+10,  2.3926e+10],\n",
      "         [ 4.2737e+23,  4.2749e+23,  4.2985e+23,  ...,  3.9118e+23,\n",
      "           3.9134e+23,  3.9095e+23],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 6.7200e+02,  7.8800e+02,  7.3400e+02,  ...,  7.5100e+02,\n",
      "           8.3200e+02,  1.0040e+03],\n",
      "         [ 6.0734e-07,  5.3174e-07,  5.1408e-07,  ...,  1.0566e-06,\n",
      "           9.2946e-07,  7.9558e-07]],\n",
      "\n",
      "        [[ 3.6906e+00,  3.9191e+00,  2.7857e+00,  ...,  6.9253e+00,\n",
      "           7.1525e+00,  8.2021e+00],\n",
      "         [ 1.6476e+07,  1.5367e+07,  1.3189e+07,  ...,  3.9362e+07,\n",
      "           3.5169e+07,  3.7860e+07],\n",
      "         [ 1.1288e+20,  1.1351e+20,  1.0916e+20,  ...,  2.2738e+20,\n",
      "           2.0138e+20,  3.0602e+20],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           1.6231e-03,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 5.6953e-07,  6.0126e-07,  7.6944e-07,  ...,  6.0391e-07,\n",
      "           1.8864e-06,  1.5731e-06]],\n",
      "\n",
      "        [[ 2.6021e+02,  2.5748e+02,  2.5316e+02,  ...,  2.2521e+02,\n",
      "           2.2275e+02,  2.1600e+02],\n",
      "         [ 3.1417e+09,  3.1040e+09,  3.0467e+09,  ...,  2.4816e+09,\n",
      "           2.4514e+09,  2.4265e+09],\n",
      "         [ 2.8892e+22,  2.8630e+22,  2.7810e+22,  ...,  2.2032e+22,\n",
      "           2.1195e+22,  2.0909e+22],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  2.0285e-04,  0.0000e+00,  ...,  2.0285e-04,\n",
      "           0.0000e+00,  2.0285e-04],\n",
      "         [ 5.0000e+00,  1.0000e+01,  2.0000e+01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-9.9999e+04, -9.9999e+04, -9.9999e+04,  ...,  5.5316e-07,\n",
      "           5.4628e-07,  5.6763e-07]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 6.1868e+02,  6.3843e+02,  6.2144e+02,  ...,  5.9984e+02,\n",
      "           5.9728e+02,  5.7981e+02],\n",
      "         [ 8.5543e+09,  8.5327e+09,  8.4378e+09,  ...,  8.6080e+09,\n",
      "           8.6050e+09,  8.4993e+09],\n",
      "         [ 1.1776e+23,  1.1816e+23,  1.1773e+23,  ...,  1.2167e+23,\n",
      "           1.2098e+23,  1.1947e+23],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 9.9000e+01,  1.5600e+02,  9.2000e+01,  ...,  3.0000e+01,\n",
      "           2.7000e+01,  2.6000e+01],\n",
      "         [ 3.9464e-07,  4.0382e-07,  4.3448e-07,  ...,  6.6596e-07,\n",
      "           6.3729e-07,  6.1831e-07]],\n",
      "\n",
      "        [[ 3.5151e+01,  3.6894e+01,  3.4356e+01,  ...,  5.1313e+01,\n",
      "           5.1322e+01,  5.1130e+01],\n",
      "         [ 2.6854e+08,  2.6365e+08,  2.6244e+08,  ...,  3.8431e+08,\n",
      "           3.9430e+08,  3.9086e+08],\n",
      "         [ 2.0885e+21,  2.0266e+21,  1.9463e+21,  ...,  2.2900e+21,\n",
      "           2.5247e+21,  2.4121e+21],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  1.7610e-04,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 4.6203e-08,  6.1296e-08,  5.5279e-08,  ...,  4.3067e-08,\n",
      "           4.4306e-08,  3.5457e-08]],\n",
      "\n",
      "        [[ 3.2748e+01,  3.2815e+01,  3.3911e+01,  ...,  2.3826e+01,\n",
      "           2.3881e+01,  2.4368e+01],\n",
      "         [ 2.2799e+08,  2.3396e+08,  2.3110e+08,  ...,  1.6103e+08,\n",
      "           1.5904e+08,  1.6553e+08],\n",
      "         [ 1.6486e+21,  1.7257e+21,  1.7494e+21,  ...,  1.3836e+21,\n",
      "           1.3977e+21,  1.4291e+21],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  3.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 4.0246e-07,  3.8591e-07,  3.8907e-07,  ...,  4.1580e-07,\n",
      "           4.0617e-07,  4.5691e-07]]])\n",
      "This is iffy\n",
      "tensor([[[-5.1555e+02],\n",
      "         [ 1.2052e+10],\n",
      "         [ 3.3754e+23],\n",
      "         ...,\n",
      "         [ 1.6306e-02],\n",
      "         [ 1.6632e+02],\n",
      "         [ 5.9183e-02]],\n",
      "\n",
      "        [[        nan],\n",
      "         [        nan],\n",
      "         [        nan],\n",
      "         ...,\n",
      "         [        nan],\n",
      "         [        nan],\n",
      "         [ 5.9180e-02]],\n",
      "\n",
      "        [[-1.0186e+02],\n",
      "         [ 1.2311e+09],\n",
      "         [ 2.0026e+22],\n",
      "         ...,\n",
      "         [ 1.6365e-02],\n",
      "         [ 7.3133e+00],\n",
      "         [-9.5164e+03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.9161e+02],\n",
      "         [ 3.5012e+09],\n",
      "         [ 8.9958e+22],\n",
      "         ...,\n",
      "         [ 1.6317e-02],\n",
      "         [ 5.2031e+01],\n",
      "         [ 5.9181e-02]],\n",
      "\n",
      "        [[-2.1585e+01],\n",
      "         [ 1.3547e+08],\n",
      "         [ 1.7474e+21],\n",
      "         ...,\n",
      "         [ 1.6306e-02],\n",
      "         [-3.8312e-03],\n",
      "         [ 5.9182e-02]],\n",
      "\n",
      "        [[-1.4804e+01],\n",
      "         [ 1.0511e+08],\n",
      "         [ 1.3849e+21],\n",
      "         ...,\n",
      "         [ 1.6452e-02],\n",
      "         [ 1.7557e+00],\n",
      "         [ 5.9181e-02]]], grad_fn=<CopySlices>)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-c020148dbda0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model1 = train(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlabels1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlabels2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-17831404d16d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, inputs, labels, weight, valSets, valLabels, valweight)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-fead6ff0a6e9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This is iffy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m33\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(model1)\n",
    "model1 = train(\n",
    "    model1,\n",
    "    torch.cat((train1, train2), dim=0),\n",
    "    labels1 + labels2,\n",
    "    [weights1[i] + weights2[i] for i in range(5)],\n",
    "    train3,\n",
    "    labels3,\n",
    "    weights3\n",
    ")\n",
    "\n",
    "print('================ here begins the training of the new model ===============')\n",
    "\n",
    "model3 = train(\n",
    "    model3,\n",
    "    torch.cat((train1, train2), dim=0),\n",
    "    labels1 + labels2,\n",
    "    [weights1[i] + weights2[i] for i in range(5)],\n",
    "    train3,\n",
    "    labels3,\n",
    "    weights3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x7f59a5bdd820>\n",
      "Linear(in_features=30, out_features=5, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(model3.parameters())\n",
    "feature_extraction = [child for child in model1.children()]\n",
    "print(feature_extraction[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
